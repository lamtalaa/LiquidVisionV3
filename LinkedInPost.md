ğŸ§  LiquidVision â€” â€œPairing Computer Vision With Real-Time Sentimentâ€

ğŸš€ The Challenge
Blending image classification with natural-language sentiment inside one fluid SwiftUI experience felt like juggling two asynchronous pipelines. I wanted the app to snap a photo, understand what it saw, and instantly tell you how people usually feel about that subject â€” all without blocking the UI.

ğŸ”§ What I Built
LiquidVision is an on-device SwiftUI app that classifies images with MobileNetV2 through Vision/Core ML, then pipes the predicted label into Appleâ€™s Natural Language framework for sentiment scoring. The latest update introduces a coordinator registry so new features can register themselves without touching `AppCoordinator`, keeping navigation wiring loosely coupled.

ğŸ“š What I Learned
Working on this release reinforced how much state discipline modern Swift concurrency expects. Highlights:
â€¢ Caching VNCoreMLModel instances in an actor to avoid repeating heavy initialization.
â€¢ Decoupling feature coordinators via type-erased factories so adding modules stays open-closed.
â€¢ Leaning on value-style `ViewState` structs to keep `@Published` state minimal and predictable.

ğŸ§° Tech Stack & Architecture
This build leans on Swift 6-era tooling with MVVM + Coordinator boundaries.
â€¢ ğŸ§© Core ML + Vision for inference
â€¢ ğŸ’¬ Natural Language for NLTagger sentiment scoring
â€¢ ğŸ—ï¸ Coordinator registry that hands out view models via cached factories
â€¢ âš™ï¸ SwiftUI + PhotosUI for the capture-to-classify UI

ğŸ¬ Visual Snippet
Picture a glassy card UI: tap to choose a photo, LiquidVision surfaces â€œGolden Retrieverâ€ at 91% confidence, and seconds later the sentiment badge fades in with â€œPositive (0.82)â€. A swipe to the Sentiment tab lets you test phrases live.

ğŸ’­ Reflection
Refactoring the coordinators was a reminder that architectural debt creeps in quietly. Abstracting view-model factories into a registry gave me confidence the next tab or experiment can plug in without rewriting the app shell.

ğŸ”— Call to Action
Curious how the coordinator registry works or want to jam on on-device ML? Dive into the repo or drop me a note â€” Iâ€™d love feedback from fellow SwiftUI + ML explorers.
ğŸ‘‰ GitHub: https://github.com/lamtalaa/LiquidVisionV2
